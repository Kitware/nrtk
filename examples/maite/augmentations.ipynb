{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a54ed21",
   "metadata": {},
   "source": [
    "# Exploring Popular Image Augmentation Libraries with MAITE\n",
    "\n",
    "This is a simple notebook exploring the current state of popular image augmentation libraries. Additionally, the usability of the Augmentation protocol from MAITE will be considered.\n",
    "\n",
    "## Table Of Contents\n",
    "\n",
    "* [Environment Setup](#environment-setup)\n",
    "* [Initial Image and Detections](#init-image-and-detections)\n",
    "* [Albumentations](#albumentations)\n",
    "  * [ReplayCompose](#replaycompose)\n",
    "* [AugLy](#augly)\n",
    "* [imgaug](#imgaug)\n",
    "* [Kornia](#kornia)\n",
    "* [Torchvision Transforms](#torchvision-transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4687d",
   "metadata": {},
   "source": [
    "## Environment Setup <a name=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92807500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # noqa: F401\n",
    "\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "print(\"Installing augly...\")\n",
    "!{sys.executable} -m pip install -q augly[image]\n",
    "print(\"Installing albumentations...\")\n",
    "!{sys.executable} -m pip install -q albumentations\n",
    "print(\"Installing imgaug...\")\n",
    "!{sys.executable} -m pip install -q imgaug\n",
    "print(\"Installing kornia...\")\n",
    "!{sys.executable} -m pip install -q kornia\n",
    "print(\"Installing torch and torchvision...\")\n",
    "!{sys.executable} -m pip install -q \"torch!=2.0.1\" \"torchvision==0.17\"\n",
    "print(\"Installing smqtk-detection...\")\n",
    "!{sys.executable} -m pip install -qU smqtk-detection[centernet]\n",
    "# Remove opencv-python, which requires libGL, which we don't require here, and replace with opencv-python-headless\n",
    "print(\"Installing headless OpenCV...\")\n",
    "!{sys.executable} -m pip uninstall -qy opencv-python opencv-python-headless  # make sure they're both gone.\n",
    "!{sys.executable} -m pip install -q opencv-python-headless\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"jpeg\"  # Use JPEG format for inline visualizations\n",
    "import os\n",
    "import random\n",
    "import urllib.request\n",
    "from collections.abc import Hashable, Iterable, Iterator\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import albumentations as a  # type: ignore\n",
    "import augly.image as imaugs  # type: ignore\n",
    "import imgaug.augmenters as iaa  # type: ignore\n",
    "import imgaug.random  # type: ignore\n",
    "import kornia as k  # type: ignore\n",
    "import numpy as np\n",
    "import torch  # type: ignore\n",
    "from augly.image import aug_np_wrapper  # type: ignore\n",
    "from imgaug.augmentables.bbs import BoundingBoxesOnImage  # type: ignore\n",
    "from kornia.augmentation import AugmentationSequential  # type: ignore\n",
    "from kornia.geometry.bbox import bbox_generator  # type: ignore\n",
    "from matplotlib import pyplot as plt  # type: ignore\n",
    "from matplotlib.axes import Axes  # type: ignore\n",
    "from matplotlib.patches import Rectangle  # type: ignore\n",
    "from PIL import Image\n",
    "from smqtk_detection.impls.detect_image_objects.centernet import CenterNetVisdrone\n",
    "from smqtk_image_io.bbox import AxisAlignedBoundingBox\n",
    "from torchvision import tv_tensors  # type: ignore\n",
    "from torchvision.transforms import v2 as tv  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25fbf5",
   "metadata": {},
   "source": [
    "## Initial Image and Detections <a name=\"init-image-and-detections\"></a>\n",
    "\n",
    "We'll perform augmentations on a singular image from Visdrone and its detections from the SMQTK-implementation of the CenterNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c266bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "img_path = os.path.join(data_dir, \"visdrone_img.jpg\")\n",
    "if not os.path.isfile(img_path):\n",
    "    _ = urllib.request.urlretrieve(\"https://data.kitware.com/api/v1/item/623880f14acac99f429fe3ca/download\", img_path)\n",
    "\n",
    "img = np.asarray(Image.open(img_path))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bfd745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download weights\n",
    "model_file = os.path.join(data_dir, \"centernet-resnet50.pth\")\n",
    "if not os.path.isfile(model_file):\n",
    "    urllib.request.urlretrieve(\"https://data.kitware.com/api/v1/item/623259f64acac99f426f21db/download\", model_file)\n",
    "\n",
    "center_net_detector = CenterNetVisdrone(\n",
    "    arch=\"resnet50\",\n",
    "    model_file=\"data/centernet-resnet50.pth\",\n",
    "    max_dets=500,\n",
    "    use_cuda=False,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bde862",
   "metadata": {},
   "source": [
    "Define a dataclass for detections that conforms to maite's [`ObjectDetectionTarget`](https://github.com/mit-ll-ai-technology/maite/blob/14b014850aff0e1715e1a60279d3eed1e1a74cfe/src/maite/_internals/protocols/object_detection.py#L27) protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ObjectDetectionData:\n",
    "    \"\"\"Dataclass for object detction data\"\"\"\n",
    "\n",
    "    boxes: np.ndarray\n",
    "    labels: np.ndarray\n",
    "    scores: np.ndarray\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "        \"\"\"Return an iterator for object detection data\"\"\"\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Get next object detection data\"\"\"\n",
    "        if self.n < len(self.boxes):\n",
    "            self.n += 1\n",
    "            return self.boxes[self.n - 1], self.labels[self.n - 1], self.scores[self.n - 1]\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736262e",
   "metadata": {},
   "source": [
    "We'll reformat our detections so they're in the format specified by the relevant MAITE protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd644014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dets_to_maite(\n",
    "    dets: Iterable[tuple[AxisAlignedBoundingBox, dict[Hashable, float]]],\n",
    "    thresh: float = 0.4,\n",
    ") -> ObjectDetectionData:\n",
    "    \"\"\"Convert detections to MAITE format\"\"\"\n",
    "    boxes = []\n",
    "    labels = []\n",
    "\n",
    "    for box, probs in dets:\n",
    "        box_out = box.min_vertex.tolist()\n",
    "        box_out.extend(box.max_vertex.tolist())\n",
    "\n",
    "        # Just keep the highest scored prediction\n",
    "        cls_name = max(probs, key=lambda key: probs[key])\n",
    "        if probs[cls_name] > thresh:\n",
    "            cls_idx = list(probs.keys()).index(cls_name)\n",
    "\n",
    "            boxes.append(box_out)\n",
    "            labels.append(cls_idx)\n",
    "\n",
    "    return ObjectDetectionData(\n",
    "        boxes=np.asarray(boxes, dtype=float),\n",
    "        labels=np.asarray(labels, dtype=int),\n",
    "        scores=np.asarray([1] * len(labels), dtype=int),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dets = list(center_net_detector([img]))[0]\n",
    "maite_detections = dets_to_maite(dets, thresh=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281d29a",
   "metadata": {},
   "source": [
    "Here we define a helper function to display the (potentially augmented) images and detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d498bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img_with_dets(\n",
    "    img: np.ndarray,\n",
    "    maite_dets: ObjectDetectionData,\n",
    "    descriptor: str,\n",
    "    show_labels: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Display img with maite detections\"\"\"\n",
    "\n",
    "    def _show_dets(ax: Axes, dets: ObjectDetectionData) -> None:\n",
    "        if len(dets.boxes) != len(dets.labels):\n",
    "            print(\"WARNING: number of detections != number of labels provided\")\n",
    "        for bbox, lbl in zip(dets.boxes, dets.labels):\n",
    "            x1 = bbox[0]\n",
    "            y1 = bbox[1]\n",
    "            x2 = bbox[2]\n",
    "            y2 = bbox[3]\n",
    "            ax.add_patch(Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor=\"r\", facecolor=\"none\"))\n",
    "\n",
    "            if show_labels:\n",
    "                ax.text(x1, y1 - 2, lbl, color=\"b\", fontsize=8)\n",
    "\n",
    "    _fig, axs = plt.subplots(figsize=(8, 8))\n",
    "    axs.set_title(descriptor)\n",
    "    axs.imshow(img)\n",
    "    axs.axis(\"off\")\n",
    "    _show_dets(axs, maite_dets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display initial images and detections\n",
    "display_img_with_dets(img, maite_detections, \"Model: SMQTK CenterNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2904d826",
   "metadata": {},
   "source": [
    "Lastly, for reproducability, we'll define a function that seeds the various random number generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb98229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed: int) -> None:\n",
    "    \"\"\"Set seeds for randomness\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.default_rng(seed)\n",
    "    imgaug.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3639102",
   "metadata": {},
   "source": [
    "## Albumentations <a name=\"albumentations\"></a>\n",
    "\n",
    "Throughout our exploration of these various augmentation tools, we will define a relatively simple pipeline, aiming to create similar pipelines across all tools. Additionally, we'll make sure each pipeline has some built-in randomness to determine the state of reproducability of each tool.\n",
    "\n",
    "[Albumentations](https://github.com/albumentations-team/albumentations) is able to simultaneously transform multiple targets, so in addition to transforming our image we can easily transform our [bounding boxes](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/), with support for multiple bounding box formats.\n",
    "\n",
    "Some discussion of Albumentations reproducability can be found [here](https://albumentations.ai/docs/examples/serialization/).\n",
    "\n",
    "It's also possible to integrate Albumentations with [PyTorch Transforms](https://albumentations.ai/docs/examples/pytorch_classification/) or [Tensorflow](https://albumentations.ai/docs/examples/tensorflow-example/).\n",
    "\n",
    "Albumentations works with NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alb_augmentation(img: np.ndarray, maite_dets: ObjectDetectionData) -> tuple[np.ndarray, ObjectDetectionData]:\n",
    "    \"\"\"Perform albumentations augmentations on input img\"\"\"\n",
    "    image = img\n",
    "    bboxes = maite_dets.boxes\n",
    "    labels = maite_dets.labels\n",
    "\n",
    "    augmentation_pipeline = a.Compose(\n",
    "        [a.HorizontalFlip(p=0.5), a.VerticalFlip(p=0.5), a.RandomBrightnessContrast(p=0.8, contrast_limit=0.4)],\n",
    "        bbox_params=a.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]),\n",
    "    )\n",
    "\n",
    "    out = augmentation_pipeline(image=image, bboxes=bboxes, class_labels=labels)\n",
    "\n",
    "    out_img, out_dets = (\n",
    "        out[\"image\"],\n",
    "        ObjectDetectionData(\n",
    "            boxes=out[\"bboxes\"],\n",
    "            labels=out[\"class_labels\"],\n",
    "            scores=np.asarray([1] * len(out[\"class_labels\"]), dtype=int),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return out_img, out_dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a30574",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "seed_all(seed)\n",
    "out_img_1, out_dets_1 = alb_augmentation(img, maite_detections)\n",
    "display_img_with_dets(out_img_1, out_dets_1, f\"Albumentations Pipeline (seed={seed})\")\n",
    "\n",
    "out_img_2, out_dets_2 = alb_augmentation(img, maite_detections)\n",
    "display_img_with_dets(out_img_2, out_dets_2, \"Albumentations Pipeline\")\n",
    "\n",
    "seed_all(seed)\n",
    "out_img_3, out_dets_3 = alb_augmentation(img, maite_detections)\n",
    "display_img_with_dets(out_img_3, out_dets_3, f\"Albumentations Pipeline (re-seeded={seed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc893cd",
   "metadata": {},
   "source": [
    "### ReplayCompose <a name=\"replaycompose\"></a>\n",
    "\n",
    "[ReplayCompose](https://albumentations.ai/docs/examples/replay/) is a tool from Albumentations that tracks augmentation parameters to reapply them to another image. Albumentations presents this as one way to apply the same parameters to multiple images, bounding boxes, etc. This usage might be a bit trickier to conform to the `Augmentation` protocol, but the tool might be useful for certain workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d235e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_pipeline = a.ReplayCompose(\n",
    "    [a.HorizontalFlip(p=0.5), a.VerticalFlip(p=0.5), a.RandomBrightnessContrast(p=0.8, contrast_limit=0.4)],\n",
    "    bbox_params=a.BboxParams(format=\"pascal_voc\", label_fields=[\"class_labels\"]),\n",
    ")\n",
    "\n",
    "image = img\n",
    "bboxes = maite_detections.boxes\n",
    "labels = maite_detections.labels\n",
    "\n",
    "replay_out = replay_pipeline(image=image, bboxes=bboxes, class_labels=labels)\n",
    "replay_out_img_1, replay_out_dets_1 = (\n",
    "    replay_out[\"image\"],\n",
    "    ObjectDetectionData(\n",
    "        boxes=replay_out[\"bboxes\"],\n",
    "        labels=replay_out[\"class_labels\"],\n",
    "        scores=np.asarray([1] * len(replay_out[\"class_labels\"]), dtype=int),\n",
    "    ),\n",
    ")\n",
    "tmp = a.ReplayCompose.replay(replay_out[\"replay\"], image=image, bboxes=bboxes, class_labels=labels)\n",
    "replay_out_img_2, replay_out_dets_2 = (\n",
    "    tmp[\"image\"],\n",
    "    ObjectDetectionData(\n",
    "        boxes=tmp[\"bboxes\"],\n",
    "        labels=tmp[\"class_labels\"],\n",
    "        scores=np.asarray([1] * len(tmp[\"class_labels\"]), dtype=int),\n",
    "    ),\n",
    ")\n",
    "\n",
    "display_img_with_dets(replay_out_img_1, replay_out_dets_1, \"Albumentations ReplayCompose Init\")\n",
    "display_img_with_dets(replay_out_img_2, replay_out_dets_2, \"Albumentations ReplayCompose Replay\")\n",
    "\n",
    "# Images should be the same\n",
    "replay_image1 = replay_out_img_1\n",
    "replay_image2 = replay_out_img_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b1d4b",
   "metadata": {},
   "source": [
    "## AugLy <a name=\"augly\"></a>\n",
    "\n",
    "Like Albumentations, [AugLy](https://github.com/facebookresearch/AugLy) can integrate with [PyTorch transformations](https://augly.readthedocs.io/en/latest/README_image.html#class-based).\n",
    "\n",
    "AugLy can also preserve [bounding box information](https://augly.readthedocs.io/en/latest/README_image.html#augmenting-structured-data) (multiple bbox formats supported).\n",
    "\n",
    "AugLy expects a PIL image as input but provides `aug_np_wrapper` which converts the numpy array to a PIL image and then calls the augmentation.\n",
    "\n",
    "The optional `metadata` argument seems to provide enough information that it would be possible to re-construct the applied augmentation, but it's not necessarily straightforward to reapply that same augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325099a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augly_augmentation(img: np.ndarray, maite_dets: ObjectDetectionData) -> tuple[np.ndarray, ObjectDetectionData]:\n",
    "    \"\"\"Perform augly augmentations on input img\"\"\"\n",
    "    image = img\n",
    "    bboxes = maite_dets.boxes\n",
    "    labels = maite_dets.labels\n",
    "\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise ValueError(\"image is not of type np.ndarray\")\n",
    "\n",
    "    augmentation_pipeline = imaugs.Compose(\n",
    "        [imaugs.HFlip(), imaugs.VFlip(), imaugs.RandomBrightness(min_factor=0.2, max_factor=2.0, p=0.5)],\n",
    "    )\n",
    "\n",
    "    meta = []\n",
    "    out = aug_np_wrapper(\n",
    "        image,\n",
    "        augmentation_pipeline,  # type: ignore\n",
    "        bboxes=bboxes,\n",
    "        bbox_format=\"pascal_voc\",\n",
    "        metadata=meta,\n",
    "    )\n",
    "\n",
    "    out_img, out_dets = (\n",
    "        out,\n",
    "        ObjectDetectionData(\n",
    "            boxes=np.asarray(meta[-1][\"dst_bboxes\"], dtype=float),\n",
    "            labels=labels,\n",
    "            scores=np.ones(labels.size),  # type: ignore\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return out_img, out_dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c85b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "seed_all(seed)\n",
    "augly_out_1, augly_dets_1 = augly_augmentation(img, maite_detections)\n",
    "display_img_with_dets(augly_out_1, augly_dets_1, f\"AugLy Pipeline (seed={seed})\")\n",
    "\n",
    "augly_out_2, augly_dets_2 = augly_augmentation(img, maite_detections)\n",
    "display_img_with_dets(augly_out_2, augly_dets_2, \"AugLy Pipeline\")\n",
    "\n",
    "seed_all(seed)\n",
    "augly_out_3, augly_dets_3 = augly_augmentation(img, maite_detections)\n",
    "display_img_with_dets(augly_out_3, augly_dets_3, f\"AugLy Pipeline (re-seeded={seed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72572f29",
   "metadata": {},
   "source": [
    "## imgaug <a name=\"imgaug\"></a>\n",
    "\n",
    "Note: at least some of imgaug transforms seem to be available via [Albumentations](https://albumentations.ai/docs/api_reference/imgaug/transforms/)\n",
    "\n",
    "While [imgaug](https://github.com/aleju/imgaug) is able to support transforming related data such as bounding boxes, bounding boxes are restricted to one format.\n",
    "\n",
    "imgaug is able to support computing (CPU) batches of augmentations including on [multiple cores](https://nbviewer.org/github/aleju/imgaug-doc/blob/master/notebooks/A03%20-%20Multicore%20Augmentation.ipynb).\n",
    "\n",
    "The documentation contains a section discussing the use of [stochastic parameters](https://imgaug.readthedocs.io/en/latest/source/parameters.html) as a means to determinism.\n",
    "\n",
    "NumPy is the supported data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4366df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imgaug_augmentation(img: np.ndarray, maite_dets: ObjectDetectionData) -> tuple[np.ndarray, ObjectDetectionData]:\n",
    "    \"\"\"Perform imgaug augmentations on input img\"\"\"\n",
    "    image = img\n",
    "    bboxes = maite_dets.boxes\n",
    "    labels = maite_dets.labels\n",
    "\n",
    "    bbs = BoundingBoxesOnImage.from_xyxy_array(bboxes, shape=image.shape)  # type: ignore\n",
    "\n",
    "    augmentation_seq = iaa.Sequential(\n",
    "        [\n",
    "            iaa.Fliplr(0.5),  # type: ignore\n",
    "            iaa.Flipud(0.5),  # type: ignore\n",
    "            iaa.MultiplyAndAddToBrightness(mul=(0.5, 1.5), add=(-30, 30)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    img_aug, bbs_aug = augmentation_seq(  # type: ignore\n",
    "        image=image,\n",
    "        bounding_boxes=bbs,\n",
    "    )\n",
    "\n",
    "    bboxes_aug = bbs_aug.to_xyxy_array()\n",
    "\n",
    "    out_img, out_dets = (\n",
    "        img_aug,\n",
    "        ObjectDetectionData(boxes=bboxes_aug, labels=labels, scores=np.ones(labels.size)),  # type: ignore\n",
    "    )\n",
    "\n",
    "    return out_img, out_dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "seed_all(seed)\n",
    "imgaug_out_1, imgaug_dets_1 = imgaug_augmentation(img, maite_detections)\n",
    "display_img_with_dets(imgaug_out_1, imgaug_dets_1, f\"imgaug Pipeline (seed={seed})\")\n",
    "\n",
    "imgaug_out_2, imgaug_dets_2 = imgaug_augmentation(img, maite_detections)\n",
    "display_img_with_dets(imgaug_out_2, imgaug_dets_2, \"imgaug Pipeline\")\n",
    "\n",
    "seed_all(seed)\n",
    "imgaug_out_3, imgaug_dets_3 = imgaug_augmentation(img, maite_detections)\n",
    "display_img_with_dets(imgaug_out_3, imgaug_dets_3, f\"imgaug Pipeline (re-seeded={seed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04871b5b",
   "metadata": {},
   "source": [
    "## Kornia <a name=\"kornia\"></a>\n",
    "\n",
    "[Kornia](https://github.com/rdevon/kornia) is able to transform bounding boxes. As a way to support rotational transforms, Kornia strongly prefers all 4 corners of the bbox as input. A function to transform from `x_min, y_min, width, height` is provided. Documentation on transforming bounding boxes and other truth objects is sparse.\n",
    "\n",
    "Kornia is able to do batched, on-GPU transformations.\n",
    "\n",
    "PyTorch tensors are the built-in data format of Kornia. Note the channel first format of Kornia augmentations. Kornia docs also frequently reference BGR vs RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kornia_augmentation(img: np.ndarray, maite_dets: ObjectDetectionData) -> tuple[np.ndarray, ObjectDetectionData]:\n",
    "    \"\"\"Perform Kornia augmentations on input img\"\"\"\n",
    "\n",
    "    def _to_kornia_bboxes(bboxes: np.ndarray) -> torch.Tensor:\n",
    "        return bbox_generator(\n",
    "            x_start=torch.Tensor(bboxes[:, 0]),\n",
    "            y_start=torch.Tensor(bboxes[:, 1]),\n",
    "            width=torch.Tensor(bboxes[:, 2] - bboxes[:, 0] + 1),\n",
    "            height=torch.Tensor(bboxes[:, 3] - bboxes[:, 1] + 1),\n",
    "        )\n",
    "\n",
    "    def _from_kornia_bboxes(bboxes: torch.Tensor) -> np.ndarray:\n",
    "        out_boxes = np.empty((0, 4))\n",
    "        for box in bboxes.numpy():\n",
    "            out_boxes = np.vstack((out_boxes, np.append(np.min(box, axis=0), np.max(box, axis=0))))\n",
    "        return out_boxes\n",
    "\n",
    "    bboxes = maite_dets.boxes\n",
    "    labels = maite_dets.labels\n",
    "\n",
    "    im = np.moveaxis(img, 2, 0)\n",
    "    image = torch.as_tensor((im - np.min(im)) / (np.max(im) - np.min(im)), dtype=torch.float32)\n",
    "    bboxes = _to_kornia_bboxes(np.array(bboxes))\n",
    "\n",
    "    augmentation_seq = AugmentationSequential(\n",
    "        k.augmentation.RandomHorizontalFlip(p=0.5),\n",
    "        k.augmentation.RandomVerticalFlip(p=0.5),\n",
    "        k.augmentation.RandomBrightness(brightness=(0.8, 1.2), clip_output=True, p=0.8),\n",
    "        data_keys=[\"input\", \"bbox\"],\n",
    "    )\n",
    "\n",
    "    out = augmentation_seq(image, bboxes)\n",
    "\n",
    "    img_out = np.moveaxis((out[0][0].numpy() * 255).astype(int), 0, 2)\n",
    "    bboxes_out = _from_kornia_bboxes(out[1])\n",
    "\n",
    "    out_img, out_dets = (\n",
    "        img_out,\n",
    "        ObjectDetectionData(boxes=bboxes_out, labels=labels, scores=np.ones(labels.size)),  # type: ignore\n",
    "    )\n",
    "\n",
    "    return out_img, out_dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3698a85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "seed_all(seed)\n",
    "kornia_img_out_1, kornia_dets_1 = kornia_augmentation(img, maite_detections)\n",
    "display_img_with_dets(kornia_img_out_1, kornia_dets_1, f\"Kornia Pipeline (seed={seed})\")\n",
    "\n",
    "kornia_img_out_2, kornia_dets_2 = kornia_augmentation(img, maite_detections)\n",
    "display_img_with_dets(kornia_img_out_2, kornia_dets_2, \"Kornia Pipeline\")\n",
    "\n",
    "seed_all(seed)\n",
    "kornia_img_out_3, kornia_dets_3 = kornia_augmentation(img, maite_detections)\n",
    "display_img_with_dets(kornia_img_out_3, kornia_dets_3, f\"Kornia Pipeline (re-seeded={seed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4d48f",
   "metadata": {},
   "source": [
    "## Torchvision Transforms v2 <a name=\"torchvision-transforms\"></a>\n",
    "\n",
    "Torchvision Transforms v1 does not have the capability to transform truth objects along with the image, but the v2 of the API aims to correct this issue. v2 is a drop-in replacement for v1. Note that the v2 of this API is still in beta.\n",
    "\n",
    "On-GPU and batched transforms are possible with torchvision.\n",
    "\n",
    "Many of torchvisions transforms expect PIL images as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d344eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torchvision_augmentation(\n",
    "    img: np.ndarray,\n",
    "    maite_dets: ObjectDetectionData,\n",
    ") -> tuple[np.ndarray, ObjectDetectionData]:\n",
    "    \"\"\"Perform torchvision augmentations on input img\"\"\"\n",
    "    bboxes = maite_dets.boxes\n",
    "    labels = maite_dets.labels\n",
    "    h, w, _ = np.shape(img)\n",
    "\n",
    "    bbs = tv_tensors.BoundingBoxes(np.array(bboxes), format=\"XYXY\", canvas_size=(h, w))  # type: ignore\n",
    "\n",
    "    image = tv.functional.to_pil_image(img)\n",
    "\n",
    "    augmentation_pipeline = tv.Compose(\n",
    "        [\n",
    "            tv.RandomHorizontalFlip(p=0.5),\n",
    "            tv.RandomVerticalFlip(p=0.5),\n",
    "            tv.ColorJitter(brightness=(0.5, 1.5), contrast=(1), saturation=(0.5, 1.5), hue=(-0.1, 0.1)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    img_aug, bbs_aug, labels_aug = augmentation_pipeline(image, bbs, labels)\n",
    "\n",
    "    out_img, out_dets = (\n",
    "        np.array(img_aug),\n",
    "        ObjectDetectionData(boxes=bbs_aug, labels=labels_aug, scores=np.asarray([1] * len(labels_aug), dtype=int)),\n",
    "    )\n",
    "\n",
    "    return out_img, out_dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "seed_all(seed)\n",
    "torchvsion_img_out_1, torchvision_dets_1 = torchvision_augmentation(img, maite_detections)\n",
    "display_img_with_dets(torchvsion_img_out_1, torchvision_dets_1, f\"Torchvision Pipeline (seed={seed})\")\n",
    "\n",
    "torchvsion_img_out_2, torchvision_dets_2 = torchvision_augmentation(img, maite_detections)\n",
    "display_img_with_dets(torchvsion_img_out_2, torchvision_dets_2, \"Torchvision Pipeline\")\n",
    "\n",
    "seed_all(seed)\n",
    "torchvsion_img_out_3, torchvision_dets_3 = torchvision_augmentation(img, maite_detections)\n",
    "display_img_with_dets(torchvsion_img_out_3, torchvision_dets_3, f\"Torchvision Pipeline (re-seeded={seed})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f8b06-8539-4f45-8910-710ccc63b45c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
