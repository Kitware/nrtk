{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cc3606",
   "metadata": {},
   "source": [
    "# Exploring the Effects of Perturbations on Saliency Map Generation\n",
    "\n",
    "The link between saliency maps and model natural robustness is currently unclear. This is a simple notebook exploring how perturbations might affect saliency maps, using tools provided by the `nrtk-jatic` and `xaitk-saliency` packages.\n",
    "\n",
    "## Table Of Contents\n",
    "* [Environment Setup](#environment-setup)\n",
    "* [Example Images](#example-images)\n",
    "* [Defining the \"Application\"](#defining-the-application)\n",
    "* [Running the \"Application\"](#running-the-application)\n",
    "    * [Classifier](#classifier)\n",
    "    * [Saliency Generator](#saliency-generator)\n",
    "    * [Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb01bbf",
   "metadata": {},
   "source": [
    "## Environment Setup <a name=\"environment-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe760de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing xaitk-jatic...\n",
      "Installing nrtk-jatic...\n",
      "Installing Hugging Face datasets...\n",
      "Installing Hugging Face transformers...\n",
      "Installing tabulate...\n",
      "Installing torch...\n",
      "Installing pybsm...\n",
      "\u001b[33mWARNING: Skipping opencv-python as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling headless OpenCV...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys  # noqa: F401\n",
    "\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "print(\"Installing xaitk-jatic...\")\n",
    "!{sys.executable} -m pip install -q xaitk-jatic\n",
    "print(\"Installing nrtk-jatic...\")\n",
    "!{sys.executable} -m pip install -q ..\n",
    "print(\"Installing Hugging Face datasets...\")\n",
    "!{sys.executable} -m pip install -q datasets\n",
    "print(\"Installing Hugging Face transformers...\")\n",
    "!{sys.executable} -m pip install -q transformers\n",
    "print(\"Installing tabulate...\")\n",
    "!{sys.executable} -m pip install -q tabulate\n",
    "print(\"Installing torch...\")\n",
    "!{sys.executable} -m pip install -q torch\n",
    "print(\"Installing pybsm...\")\n",
    "!{sys.executable} -m pip install -q pybsm\n",
    "\n",
    "# Remove opencv-python, which requires libGL, which we don't require here, and replace with opencv-python-headless\n",
    "!{sys.executable} -m pip uninstall -qy opencv-python opencv-python-headless  # make sure they're both gone.\n",
    "print(\"Installing headless OpenCV...\")\n",
    "!{sys.executable} -m pip install -q opencv-python-headless\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4a7e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local/KHQ/stephen.crowell/.cache/pypoetry/virtualenvs/nrtk-jatic-nEm8g3dX-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"jpeg\"  # Use JPEG format for inline visualizations\n",
    "from collections.abc import Hashable\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset  # type: ignore\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3db250",
   "metadata": {},
   "source": [
    "## Example Images <a name=\"example-images\"></a>\n",
    "\n",
    "We'll use example images from the CIFAR-10 test dataset, but this could be expanded to many images -- even across a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7408c951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"cifar10\", split=\"test\")\n",
    "labels = data.features[\"label\"].names  # type: ignore\n",
    "\n",
    "data.set_transform(lambda x: {\"image\": x[\"img\"], \"label\": x[\"label\"]})  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155762ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 2\n",
    "imgs = np.asarray([np.asarray(data[idx][\"image\"]) for idx in range(num_samples)])  # type: ignore\n",
    "ground_truth: list[int] = [data[idx][\"label\"] for idx in range(num_samples)]  # type: ignore\n",
    "\n",
    "for img, gt in zip(imgs, ground_truth):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.xlabel(f\"GT: {labels[gt]}\")\n",
    "    _ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6a085",
   "metadata": {},
   "source": [
    "## Defining the \"Application\" <a name=\"defining-the-application\"></a>\n",
    "\n",
    "First we'll define a couple of dataclasses to keep track of results more easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PerturbationResult:\n",
    "    \"\"\"Dataclass for storing perturbed image and associated results\"\"\"\n",
    "\n",
    "    descriptor: str\n",
    "    img: np.ndarray\n",
    "    sal_maps: np.ndarray\n",
    "    pred_class: int\n",
    "    pred_prob: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SaliencyResults:\n",
    "    \"\"\"Dataclass for storing saliency map and associated results\"\"\"\n",
    "\n",
    "    ref_img: np.ndarray\n",
    "    ref_sal_maps: np.ndarray\n",
    "    gt: int\n",
    "    pred_class: int\n",
    "    pred_prob: float\n",
    "    perturbations: list[PerturbationResult] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac353fc",
   "metadata": {},
   "source": [
    "Next, we'll define a function to compute specified metrics upon our saliency map results. These metrics include measures such as the entropy of the resulting saliency map, as well as various measures of correlation between the saliency map computed on the original image and the saliency maps computed on perturbed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b4aeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_entropy(sal_map: np.ndarray, clip_min: Optional[int] = None, clip_max: Optional[int] = None) -> float:\n",
    "    if clip_min is not None or clip_max is not None:\n",
    "        s = np.clip(sal_map, clip_min, clip_max)\n",
    "    else:\n",
    "        s = (sal_map - sal_map.min()) / (sal_map.max() - sal_map.min())\n",
    "    return entropy(s.ravel(), base=2)  # type: ignore\n",
    "\n",
    "\n",
    "def _compute_ssd(sal_map: np.ndarray, ref_sal_map: np.ndarray) -> float:\n",
    "    sum_sq_diff = np.sum(np.power(np.subtract(sal_map, ref_sal_map), 2))\n",
    "    norm = np.sqrt(np.sum(np.power(sal_map, 2)) * np.sum(np.power(ref_sal_map, 2)))\n",
    "    if not norm:\n",
    "        return np.inf\n",
    "    return sum_sq_diff / norm\n",
    "\n",
    "\n",
    "def _compute_xcorr(sal_map: np.ndarray, ref_sal_map: np.ndarray) -> float:\n",
    "    def _normalize(s: np.ndarray) -> tuple[np.ndarray, bool]:\n",
    "        s -= s.mean()\n",
    "        std = s.std()\n",
    "\n",
    "        if std:\n",
    "            s /= std\n",
    "\n",
    "        return s, std == 0\n",
    "\n",
    "    s1, c1 = _normalize(sal_map.copy())\n",
    "    s2, c2 = _normalize(ref_sal_map.copy())\n",
    "\n",
    "    if c1 and not c2:\n",
    "        return 0.0\n",
    "    return np.corrcoef(s1.flatten(), s2.flatten())[0, 1]\n",
    "\n",
    "\n",
    "def _compute_metric(sal_map: np.ndarray, ref_sal_map: np.ndarray, m: str) -> float:\n",
    "    if \"entropy\" in m:\n",
    "        _compute_entropy_setup(sal_map, m)\n",
    "    if m == \"ssd\":\n",
    "        return _compute_ssd(sal_map, ref_sal_map)\n",
    "    if m == \"xcorr\":\n",
    "        return _compute_xcorr(sal_map, ref_sal_map)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def _compute_entropy_setup(sal_map: np.ndarray, m: str) -> float:\n",
    "    if m == \"entropy\":\n",
    "        return _compute_entropy(sal_map)\n",
    "    if m == \"pos saliency entropy\":\n",
    "        return _compute_entropy(sal_map, 0, 1)\n",
    "    if m == \"neg saliency entropy\":\n",
    "        return _compute_entropy(sal_map, -1, 0)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def _generate_row(\n",
    "    sal_map: np.ndarray,\n",
    "    ref_sal_map: np.ndarray,\n",
    "    row_label: str,\n",
    "    metrics: tuple[str, ...],\n",
    ") -> list[Hashable]:\n",
    "    r: list[Hashable] = [row_label]\n",
    "\n",
    "    r.extend([_compute_metric(sal_map, ref_sal_map, metric.lower().strip()) for metric in metrics])\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "def compute_metrics(results: list[SaliencyResults], metrics: tuple[str, ...]) -> None:\n",
    "    \"\"\"Compute metrics for saliency mpas\"\"\"\n",
    "    headers = [\"Perturbation\"]\n",
    "    headers.extend(list(metrics))\n",
    "    for res in results:\n",
    "        rows = []\n",
    "\n",
    "        rows.append(_generate_row(res.ref_sal_maps[res.gt], res.ref_sal_maps[res.gt], \"Ref Image\", metrics))\n",
    "        rows.extend(\n",
    "            [\n",
    "                _generate_row(pert.sal_maps[res.gt], res.ref_sal_maps[res.gt], f\"{pert.descriptor}\", metrics)\n",
    "                for pert in res.perturbations\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        print(tabulate(rows, headers=headers, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e14885",
   "metadata": {},
   "source": [
    "We'll also define a function to display all generated saliency maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61ae8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _plot_img(img: np.ndarray, num_cols: int, descriptor: str = \"\") -> None:\n",
    "#     plt.subplot(2, num_cols, 1)\n",
    "#     plt.imshow(img, cmap=\"gray\")\n",
    "#     plt.xticks(())\n",
    "#     plt.yticks(())\n",
    "#     plt.xlabel(descriptor)\n",
    "\n",
    "# def _plot_rows(sal_maps: np.ndarray, num_cols: int, plot_idxes: Optional[list[int]] = None) -> None:\n",
    "#     plot_idxes = list(range(len(sal_maps))) if plot_idxes is None else [*set(plot_idxes)]\n",
    "#     n_cols = min(num_cols - 1, len(plot_idxes))\n",
    "#     n_rows = 2\n",
    "\n",
    "#     num_imgs = 0\n",
    "#     for r in range(n_rows):\n",
    "#         col_offset = 2\n",
    "#         if r > 0:\n",
    "#             col_offset = 3\n",
    "#         for c in range(r * n_cols, min(r * n_cols + n_cols, len(plot_idxes))):\n",
    "#             plt.subplot(n_rows, num_cols, c + col_offset)\n",
    "#             im = plt.imshow(sal_maps[plot_idxes[c]], cmap=plt.cm.RdBu, vmin=-1, vmax=1)  # type: ignore\n",
    "#             plt.xticks(())\n",
    "#             plt.yticks(())\n",
    "#             plt.xlabel(f\"{labels[plot_idxes[c]]}\")\n",
    "#             num_imgs += 1\n",
    "\n",
    "#             if num_imgs == len(plot_idxes):\n",
    "#                 fig = plt.gcf()\n",
    "#                 cax = fig.add_axes([0.38, 0.60, 0.01, 0.21])  # type: ignore # tweaked for this particular example\n",
    "#                 plt.colorbar(im, cax=cax)\n",
    "\n",
    "# def display_results(results: list[SaliencyResults], labels: Sequence[Hashable]) -> None:\n",
    "#     \"\"\"Displays saliency results with plots and a table\"\"\"\n",
    "#     num_classes = len(labels)\n",
    "\n",
    "#     for res in results:\n",
    "#         plt.figure(figsize=(10, 5))\n",
    "#         num_cols = np.ceil(num_classes / 2).astype(int) + 1\n",
    "#         pred = f\"{labels[res.pred_class]} ({res.pred_prob:.2f})\"\n",
    "#         _plot_img(res.ref_img, num_cols, f\"Ref Img\\nGT: {labels[res.gt]}\\nPred: {pred}\")\n",
    "#         _plot_rows(res.ref_sal_maps, num_cols, [res.gt])\n",
    "\n",
    "#         for pert in res.perturbations:\n",
    "#             plt.figure(figsize=(10, 5))\n",
    "#             pred = f\"{labels[pert.pred_class]} ({pert.pred_prob:.2f})\"\n",
    "#             _plot_img(pert.img, num_cols, f\"{pert.descriptor}\\nPred: {pred}\")\n",
    "#             _plot_rows(pert.sal_maps, num_cols, [res.gt])\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a73de0",
   "metadata": {},
   "source": [
    "Finally, we'll define the \"application\" which perturbs the given input image(s) to varying degrees and generates saliency maps. In this case, we'll perturb the images using a pyBSM based perturber. To easily apply this perturbation, we'll use the `JitterOTFPerturber` provided by `nrtk-jatic`, which simulates varying amounts of sensor jitter on image collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9943de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _max_class(probs: dict) -> str:\n",
    "#     v = list(probs.values())\n",
    "#     k = list(probs.keys())\n",
    "#     return k[v.index(max(v))]\n",
    "\n",
    "# def _generate_augmented_maps(\n",
    "#     idx: int,\n",
    "#     additional_params: list[dict[str, Any]],\n",
    "#     res: SaliencyResults,\n",
    "#     img: np.ndarray,\n",
    "#     num_images: int,\n",
    "#     image_classifier: ClassifyImage,\n",
    "#     saliency_generator: GenerateImageClassifierBlackboxSaliency,\n",
    "# ) -> None:\n",
    "#     for k in additional_params:\n",
    "#         print(f\"Generating saliency maps for s_y={k['s_y']} (ref image {idx+1} of {num_images})\")\n",
    "#         xform = JitterOTFPerturber(**k)\n",
    "#         img_out = xform(np.copy(img))\n",
    "#         sal_maps = saliency_generator(img_out, image_classifier)\n",
    "#         probs = next(image_classifier.classify_images(np.expand_dims(img_out, axis=0)))  # type: ignore\n",
    "#         pred_class = _max_class(probs)\n",
    "\n",
    "#         pert = PerturbationResult(\n",
    "#             descriptor=f\"ksize={k}\",\n",
    "#             img=img_out,\n",
    "#             sal_maps=sal_maps,\n",
    "#             pred_class=labels.index(pred_class),\n",
    "#             pred_prob=probs[pred_class],\n",
    "#         )\n",
    "\n",
    "#         res.perturbations.append(pert)\n",
    "\n",
    "# def generate_perturbed_sal_maps(\n",
    "#     images: np.ndarray,\n",
    "#     ground_truth: list[int],\n",
    "#     image_classifier: ClassifyImage,\n",
    "#     saliency_generator: GenerateImageClassifierBlackboxSaliency,\n",
    "#     additional_params: list[dict[str, Any]],\n",
    "#     display_maps: bool = True,\n",
    "#     metrics: tuple[str, ...] = (\"Pos Saliency Entropy\", \"Neg Saliency Entropy\", \"Entropy\", \"SSD\", \"XCorr\"),\n",
    "# ) -> list[SaliencyResults]:\n",
    "#     \"\"\"Generate saliency maps for image\"\"\"\n",
    "#     # Get class labels\n",
    "#     labels = image_classifier.get_labels()\n",
    "\n",
    "#     # Generate saliency maps\n",
    "#     results = list()\n",
    "#     for idx, img in enumerate(images):\n",
    "#         print(f\"Generating saliency maps for reference image (image {idx+1} of {len(images)})\")\n",
    "#         sal_maps = saliency_generator(img, image_classifier)\n",
    "#         probs = next(image_classifier.classify_images(np.expand_dims(img, axis=0)))\n",
    "#         pred_class = _max_class(probs)\n",
    "#         res = SaliencyResults(\n",
    "#             ref_img=np.copy(img),\n",
    "#             ref_sal_maps=sal_maps,\n",
    "#             gt=ground_truth[idx],\n",
    "#             pred_class=labels.index(pred_class),\n",
    "#             pred_prob=probs[pred_class],\n",
    "#         )\n",
    "\n",
    "#         _generate_augmented_maps(idx, additional_params, res, img, len(images), image_classifier, saliency_generator)\n",
    "\n",
    "#         results.append(res)\n",
    "\n",
    "#     for result in results:\n",
    "#         # Plot each image in set with saliency maps\n",
    "#         if display_maps:\n",
    "#             display_results([result], labels)\n",
    "\n",
    "#         # Compute metrics\n",
    "#         compute_metrics([result], metrics)\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d3103",
   "metadata": {},
   "source": [
    "## Running the \"Application\" <a name=\"running-the-application\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4cba6c",
   "metadata": {},
   "source": [
    "### Classifier <a name=\"classifier\"></a>\n",
    "\n",
    "We'll use a Hugging Face model conforming to the `maite` image classification protocol, along with the relevant `xaitk-saliency` adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "610d5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class HuggingFaceClassifier:\n",
    "#     \"\"\"MAITE wrapper for HuggingFaceClassifier\"\"\"\n",
    "\n",
    "#     def __init__(self, model_name: str, device: str) -> None:\n",
    "#         \"\"\"Initialize HuggingFaceClassifier\"\"\"\n",
    "#         self.image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "#         self.model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "#         self.device = device\n",
    "\n",
    "#         self.model.eval()\n",
    "#         self.model.to(device)\n",
    "\n",
    "#     def id2label(self) -> dict[int, Hashable]:\n",
    "#         \"\"\"Return mapping from id to label for model\"\"\"\n",
    "#         return self.model.config.id2label\n",
    "\n",
    "#     def __call__(self, batch: ic.InputBatchType) -> ic.TargetBatchType:\n",
    "#         \"\"\"Run classifier for batch and return results\"\"\"\n",
    "#         # tensor bridging\n",
    "#         input_tensor = torch.as_tensor(batch)\n",
    "#         if input_tensor.ndim != 4:\n",
    "#             raise ValueError(f\"Invalid input dimensions. Expected 4, got {input_tensor.ndim}\")\n",
    "\n",
    "\n",
    "#         # preprocess\n",
    "#         hf_inputs = self.image_processor(input_tensor, return_tensors=\"pt\")\n",
    "\n",
    "#         # put on device\n",
    "#         hf_inputs = hf_inputs.to(self.device)\n",
    "\n",
    "#         # get predictions\n",
    "#         with torch.no_grad():\n",
    "#             return self.model(**hf_inputs).logits.softmax(1).detach().cpu()\n",
    "\n",
    "\n",
    "# jatic_classifier: ic.Model = HuggingFaceClassifier(\n",
    "#     model_name=\"aaraki/vit-base-patch16-224-in21k-finetuned-cifar10\",\n",
    "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d769060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = JATICImageClassifier(classifier=jatic_classifier, id_to_name=jatic_classifier.id2label())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e20aa9d",
   "metadata": {},
   "source": [
    "### Saliency Generator <a name=\"saliency-generator\"></a>\n",
    "\n",
    "We'll use the `SlidingWindowStack` blackbox saliency generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3662eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sal_generator = SlidingWindowStack(window_size=(2, 2), stride=(1, 1), threads=4)\n",
    "# sal_generator.fill = (128, 128, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599843c4",
   "metadata": {},
   "source": [
    "### Results <a name=\"results\"></a>\n",
    "\n",
    "Note: for clarity, we'll only be performing the saliency analysis with respect to the groundtruth class, but this analysis could also be applied to the predicted class, which may be useful in cases where the groundtruth and predictions may differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b07728a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = generate_perturbed_sal_maps(\n",
    "#     images=imgs,\n",
    "#     ground_truth=ground_truth,\n",
    "#     image_classifier=classifier,\n",
    "#     saliency_generator=sal_generator,\n",
    "#     additional_params=[{\"s_y\": 1e-4, \"s_x\": 0.0}, {\"s_y\": 1.2e-4, \"s_x\": 0.0}, {\"s_y\": 1.4e-4, \"s_x\": 0.0}],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0dc95",
   "metadata": {},
   "source": [
    "We can visually see that as the quality of the input image degrades (more perturbation), the quality of the generated saliency maps similarly degrades.\n",
    "\n",
    "In an attempt to quantify these differences, we've also computed several metrics:\n",
    "\n",
    "#### Entropy <a name=\"entropy\"></a>\n",
    "\n",
    "If we compare entropy values using both positive and negative saliency we don't see much of a change across degradations. This is likely due to negative and positive saliency \"fighting\" each other as degradation increases (as one increases, the other decreases).\n",
    "\n",
    "If we consider entropy values computed from only positive or only negative saliency values, we see differences in values. Looking at the dominant saliency type (i.e. positive/blue) for the ship, we can see that as degradation gets worse, entropy increases -- to a certain point. The reduction in entropy likely corresponds to the classifier being less able to identify key features that led to the original probability distribution for the reference image due to the degradion. Eventually these features may become so degraded that the classifier begins predicting with very low confidence. Looking at the domainant saliency type (i.e. negative/red) for the cat, we see similar changes as the other reference image; however, the positive and negative saliencies are significantly closer. This could be a sign of less definitve features used for identificantion. Since this does not c\n",
    "\n",
    "If we look at the opposite saliency type for each reference image, we see a very slight decrease in entropy as degradation increases. This potentially indicates that the degradation introduces noise that the classifier misidentifies as a contraindicator for the ground truth class, but more likely corresponds to the classifier predicting with less confidence due to the loss in higher quality features.\n",
    "\n",
    "#### Sum of Squared Differences (SSD) <a name=\"SSD\"></a> \n",
    "(0 is most similar) The sum of squared differences lets us quantitatively confirm that as degradation gets worse, saliency maps are increasiningly dissimilar to the original reference saliency map. However, the metric doesn't give us much insight into what is actually happening to create these differences.\n",
    "\n",
    "#### Cross-Correlation (XCorr) <a name=\"XCorr\"></a>  \n",
    "(1 is most similar) Cross-correlations tell us similar information as SSD. The introduction of negative correlation values, however, potentially indicates that the saliency maps begin to become the \"opposite\" of the original reference saliency maps. The aligns with the pattern we saw with positive/negative saliency entropy -- we see an introduction of the opposite saliency as the image becomes more degraded and the classifier becomes more confused. The likely doesn't occur with the ship reference image as it was more strongly salient in one direction compared to the cat reference image was contained a more balanced mix of both positive and negative saliency.\n",
    "\n",
    "While aspects of model explainability and model robustness have previously been studied independently, this notebook provides a preliminary exploration of their relationship through quantification of how saliency maps change due to various perturbations. Future work will explore whether quantitative changes in the structure and quality of saliency maps provide a mechanism for understanding model failure modes and edge cases due to various perturbations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
