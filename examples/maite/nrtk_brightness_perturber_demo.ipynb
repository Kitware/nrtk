{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b1eb19-3148-46ca-8a9a-fdf18bd2b18d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is part of the NRTK demonstration suite, demonstrating how perturbations can be applied and their impact measured via MAITE evaluation workflows.\n",
    "\n",
    "## Layout\n",
    "This notebook demonstrates how a particular condition (in this case, <b><i>extreme illumination conditions</i></b>), can affect an object detection model, and how that impact can be measured. The overall structure is:\n",
    "\n",
    "- **Traditional vs. relative mAP:**\n",
    "    - An overview of the nuances of what we'll be evaluating.\n",
    "- **Setup:**\n",
    "    - Notebook initialization, loading the supporting python code. Depending on if this is the first time you've run this notebook, this may take some time.\n",
    "    - Loading the source image, which will be used throughout the notebook.\n",
    "- **Image perturbation examples:**\n",
    "    - The NRTK perturbation is demonstrated on the source image.\n",
    "- **Baseline detections:**\n",
    "    - The object detection model is loaded and run on the unperturbed image. These will serve as \\\"ground truth\\\" for comparisons against the perturbed images.\n",
    " \n",
    "At this point, we have the fundamental elements of our evaluation: the model, our reference image, and a mechanism for creating the perturbed test images. Next we adapt these elements to be used with the MAITE evaluation workflow:\n",
    "\n",
    "- **Wrapping the detection model**\n",
    "- **Wrapping the reference image as a dataset**\n",
    "- **Wrapping the perturbation as augmentation objects**\n",
    "- **Wrapping the metrics**\n",
    "\n",
    "After the evaluation elements have been wrapped, we can run the evaluation:\n",
    "\n",
    "- **Preparing the augmentations:**\n",
    "    - We specify the range of perturbation values to evaluate and optionally specify which ones we'd like to visualize.\n",
    "- **Evaluation of augmented data:**\n",
    "    - Each augmentation is run through MAITE's evaluation workflow, computing the mean average precision metric relative to the unperturbed detections.\n",
    "- **Evaluation analysis:**\n",
    "    - We plot and discuss the mAP@50 metric from each of the perturbed images, as well as per-class and per-area results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e1964-46e2-4497-88ff-274eaca17960",
   "metadata": {},
   "source": [
    "# Evaluation guidance: traditional vs. relative mAP\n",
    "\n",
    "This notebook will be evaluating the perturbed images using mean average precision (mAP) **relative** to detections from the unperturbed image. Traditional mAP scores the computed detections to ground-truth annotations vetted by an analyst; the mAP metric indicates how well the detector does compared to that analyst and thus measures the detector's \"absolute\" performance (\"absolute\" in the sense that the assumption is no detector can do better than the analyst.)\n",
    "\n",
    "In contrast, in this notebook, we're not concerned with the **absolute** ability of the detector to find objects of interest. Rather, we're interested in how the **perturbations** affect the detector *relative to the unperturbed image*. It's expected that the detector won't find every target in the unperturbed image; instead, we're measuring the **change in the detections** (or classifications) caused by the perturbations.\n",
    "\n",
    "To support relative mAP, we'll be computing detections on the unperturbed image and using those as our \"ground truth\" dataset, and using the MAITE dataset class slightly differently than usual. For example, there's no on-disk json file of reference annotations with an associated data loader; instead, we'll be taking the computed detections and manually copying them over into the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9268412-0d13-4fed-ae78-f6ee02c94c9c",
   "metadata": {},
   "source": [
    "# Setup: Notebook initialization\n",
    "The next few cells import the python packages used in the rest of the notebook.\n",
    "\n",
    "**Note:** We are suppressing warnings within this notebook to reduce visual clutter for demonstration purposes. If any issues arise while executing this notebook, we recommend that the first cell is **not** executed so that any related warnings are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796743f8-2f1e-42aa-ad6b-f71e721602f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "# warning suppression\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # noqa: F401\n",
    "\n",
    "print(\"Beginning package installation...\")\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "\n",
    "print(\"Installing required packages...\")\n",
    "!{sys.executable} -m pip install -q \"matplotlib\" --no-cache-dir\n",
    "!{sys.executable} -m pip install -q \"torchvision\" --no-cache-dir\n",
    "!{sys.executable} -m pip install -q \"torchmetrics\" --no-cache-dir\n",
    "!{sys.executable} -m pip install -q \"ultralytics\" --no-cache-dir\n",
    "\n",
    "# OpenCV must be uninstalled and reinstalled last due to other packages installing OpenCV\n",
    "print(\"Doing a fresh install of opencv-python-headless...\")\n",
    "!{sys.executable} -m pip uninstall -qy \"opencv-python\" \"opencv-python-headless\"\n",
    "!{sys.executable} -m pip install -q \"opencv-python-headless\" --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e6bd1-1c56-447a-b1e4-16df26269ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from collections.abc import Sequence\n",
    "from typing import Any, Callable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# some initial imports\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"jpeg\"  # Use JPEG format for inline visualizations\n",
    "\n",
    "from matplotlib import pyplot as plt  # type: ignore\n",
    "from PIL import Image\n",
    "\n",
    "from nrtk.impls.perturb_image.generic.PIL.enhance import BrightnessPerturber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e9bfa-fcea-4a1d-a8e0-9455b28777a2",
   "metadata": {},
   "source": [
    "# Setup: Source image\n",
    "\n",
    "In the next cell, we'll download and display a source image from the __[VisDrone](https://github.com/VisDrone/VisDrone-Dataset)__ dataset. The image will be cached in a local `data` subdirectory.\n",
    "\n",
    "### A note on image storage\n",
    "\n",
    "Typically in ML workflows, batches of images are processed as tensors of the color channels. Both our perturber (NRTK) and object detector (YOLO) accept numpy `ndarray` objects, and we will use [matplotlib.imshow](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) to view them. The complication is that although YOLO inferences on `ndarray`, [it expects the color channels to be in BGR](https://docs.ultralytics.com/modes/predict) order. If we naively view the same data YOLO inferences on, the colors will be wrong; if we naively inference on what we view, the detections will be wrong. (Our NRTK perturbation is agnostic to the channel order.)\n",
    "\n",
    "In this notebook, we'll convert the channel order to BGR when we load, and convert back whenever we explicitly call `imshow`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57facb8b-4cb6-476b-a321-f48d70e773ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nrtk\n",
    "\n",
    "print(nrtk.__version__)\n",
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "img_path = os.path.join(data_dir, \"visdrone_img.jpg\")\n",
    "if not os.path.isfile(img_path):\n",
    "    url = \"https://data.kitware.com/api/v1/item/623880f14acac99f429fe3ca/download\"\n",
    "    _ = urllib.request.urlretrieve(url, img_path)  # noqa: S310\n",
    "\n",
    "img_pil = Image.open(img_path)\n",
    "img_nd_bgr = np.asarray(img_pil)[\n",
    "    :,\n",
    "    :,\n",
    "    ::-1,\n",
    "]  # tip o' the hat to https://stackoverflow.com/questions/4661557/pil-rotate-image-colors-bgr-rgb\n",
    "plt.figure()\n",
    "plt.axis(\"off\")\n",
    "\n",
    "_ = plt.imshow(img_nd_bgr[:, :, ::-1])  # explicitly changing BGR to RGB for imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2e2a3-a3a2-4101-9872-bae618effab0",
   "metadata": {},
   "source": [
    "# NRTK Brightness perturbation: examples and guidance\n",
    "\n",
    "The Brightness perturbation is set by a single floating point value `f`:\n",
    "\n",
    "- `f == 0.0` (the minimum value): returns a black image.\n",
    "- `0.0 < f < 1.0`: returns an image dimmer than the original.\n",
    "- `f == 1.0`: returns the original image unchanged.\n",
    "- `f > 1.0`: returns an image brighter than the original. There is no upper bound, but values greater than 2 or 3 start to wash out objects in a typical image, as seen below.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a370d2a-2489-4c48-b29a-ea8ad3ef98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(2, 4, figsize=(10, 4))\n",
    "for idx, f in enumerate((0.0, 0.25, 0.6, 1.0, 1.5, 2, 3, 6)):\n",
    "    (row, col) = (int(idx / 4), idx % 4)\n",
    "    bp = BrightnessPerturber(factor=f)\n",
    "    ax[row, col].set_title(f\"factor: {f}\")\n",
    "    ax[row, col].imshow(bp(img_nd_bgr)[0][:, :, ::-1])\n",
    "    _ = ax[row, col].axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb653b-6be9-4d58-8d69-db4b64c3d81c",
   "metadata": {},
   "source": [
    "# Baseline detections\n",
    "\n",
    "In the next cell, we'll download a [YOLOv11](https://docs.ultralytics.com/models/yolo11/) model, compute object detections on the source image, and display the results. As discussed above, these detections will serve as the \"ground truth\" for our relative mAP evaluation later.\n",
    "\n",
    "*Note that here, we're using YOLO's built-in visualization tool, which automatically adjusts for BGR / RGB order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55600de0-089f-4dd6-ac4d-7896b8df5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import YOLO support\n",
    "import ultralytics\n",
    "\n",
    "ultralytics.checks()\n",
    "print(\"Downloading model...\")\n",
    "model = ultralytics.YOLO(\"yolo11n.pt\")\n",
    "print(\"Computing baseline...\")\n",
    "baseline = model(img_nd_bgr)\n",
    "baseline[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39050929-bf48-44b1-9247-5cac7f44cd8c",
   "metadata": {},
   "source": [
    "# MAITE Evaluation workflow preparation\n",
    "\n",
    "We'll use the [MAITE Evaluation workflow](https://jatic.pages.jatic.net/cdao/maite/generated/maite.workflows.evaluate.html) to evaluate the performance of the perturbed data against our baseline detections. We'll need to \"wrap\" our model, data, and perturbations into callable objects to pass to the `maite.workflows.evaluate` function:\n",
    "\n",
    "- We'll wrap the **model** to make predictions on input data when called.\n",
    "\n",
    "- The wrapped **dataset** will return our test image when called. Note that this will be the original, unperturbed image; we'll apply our perturbations via...\n",
    "\n",
    "- ...the **augmentation** object, which applies the perturbation to the image inside the evaluation.\n",
    "\n",
    "- Finally, the **metric** object will define our precise scoring methodology.\n",
    "\n",
    "The evaluation workflow in this notebook is slightly unusual. Typical ML workflows apply many different augmentations / perturbations to much larger datasets, and only call `evaluate` once to get a statistical view of performance. But since the goal of this notebook is to drill down into how perturbation affects performance, we've essentially flipped process, calling `evaluate` (and thus our wrapped objects) many times, once per loop on our single image perturbed to a known degree, and then observing how the metrics respond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da908c7f-b499-4361-8e2b-cf7647b45de6",
   "metadata": {},
   "source": [
    "## Some helper classes\n",
    "\n",
    "The following cell adds two classes to allow us to use YOLO detections with the MAITE evaluation workflow:\n",
    "\n",
    "1. The `YOLODetectionTarget` helper class that stores the bounding boxes, label indices, and confidence scores for a single image's detections.\n",
    "\n",
    "2. The `MaiteYOLODetection` adapter class that conforms to the MAITE [Object Detection Dataset](https://jatic.pages.jatic.net/cdao/maite/generated/maite.protocols.object_detection.Dataset.html) protocol by providing the `__len__` and `__getitem__` methods. The returned item is a tuple of (image, `YOLODetectionTarget`, metadata-dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "\n",
    "from nrtk.interop.maite.interop.object_detection.dataset import JATICObjectDetectionDataset\n",
    "\n",
    "##\n",
    "## Helper class for containing the boxes, label indices, and confidence scores.\n",
    "##\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class YOLODetectionTarget:\n",
    "    \"\"\"\n",
    "    A helper class to represent object detection results in the format expected by YOLO-based models.\n",
    "\n",
    "    Attributes:\n",
    "        boxes (torch.Tensor): A tensor containing the bounding boxes for detected objects in\n",
    "            [x_min, y_min, x_max, y_max] format.\n",
    "        labels (torch.Tensor): A tensor containing the class labels for the detected objects.\n",
    "            These may be floats for compatibility with specific datasets or tools.\n",
    "        scores (torch.Tensor): A tensor containing the confidence scores for the detected objects.\n",
    "    \"\"\"\n",
    "\n",
    "    boxes: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    scores: torch.Tensor\n",
    "\n",
    "\n",
    "##\n",
    "## Prepare results for ingestion into maite dataset by puttin them into detection object\n",
    "## Images must be channel first (c, h, w) in maite dataset objects\n",
    "##\n",
    "imgs = [np.transpose(img_nd_bgr, (2, 0, 1))]\n",
    "dets = []\n",
    "metadata = [{\"id\": 0}]\n",
    "for _detection in baseline:\n",
    "    boxes = baseline[0].boxes.xyxy.cpu()\n",
    "    labels = baseline[0].boxes.cls.cpu()  # note, these are floats, not ints\n",
    "    scores = baseline[0].boxes.conf.cpu()\n",
    "\n",
    "    dets.append(YOLODetectionTarget(boxes, labels, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cd8fb3-2ea6-492c-ab84-f3432b9d3440",
   "metadata": {},
   "source": [
    "## (1) Wrapping the detection model\n",
    "\n",
    "The first object we'll wrap will be the detection model. The cell below defines a class adapting YOLO for the [MAITE Object Detection Model](https://jatic.pages.jatic.net/cdao/maite/generated/maite.protocols.object_detection.Model.html) protocol. The `__call__` method runs the model on images in the batch and is called by the MAITE evaluation workflow later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1131aa7-3716-479f-838b-0464b74106e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import maite.protocols.object_detection as od\n",
    "from maite.protocols import ArrayLike\n",
    "\n",
    "\n",
    "class MaiteYOLODetector:\n",
    "    \"\"\"\n",
    "    A wrapper class for a YOLO model to simplify its usage with input batches and object detection targets.\n",
    "\n",
    "    This class takes a YOLO model instance, processes input image batches, and converts predictions into\n",
    "    `YOLODetectionTarget` instances.\n",
    "\n",
    "    Attributes:\n",
    "        _model (ultralytics.models.yolo.model.YOLO): The YOLO model instance used for predictions.\n",
    "\n",
    "    Methods:\n",
    "        __call__(batch):\n",
    "            Processes a batch of images through the YOLO model and returns the predictions as\n",
    "            `YOLODetectionTarget` instances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: ultralytics.models.yolo.model.YOLO) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the MaiteYOLODetector with a YOLO model instance.\n",
    "\n",
    "        Args:\n",
    "            model (ultralytics.models.yolo.model.YOLO): The YOLO model to use for predictions.\n",
    "        \"\"\"\n",
    "        self._model = model\n",
    "\n",
    "    def __call__(self, batch: Sequence[ArrayLike]) -> Sequence[YOLODetectionTarget]:\n",
    "        \"\"\"\n",
    "        Processes a batch of images using the YOLO model and converts the predictions to `YOLODetectionTarget`\n",
    "        instances.\n",
    "\n",
    "        Args:\n",
    "            batch (Sequence[ArrayLike]): A batch of images in (c, h, w) format (channel-first).\n",
    "\n",
    "        Returns:\n",
    "            Sequence[YOLODetectionTarget]: A list of YOLODetectionTarget instances containing the predictions for each\n",
    "            image in the batch.\n",
    "        \"\"\"\n",
    "        for i in range(len(batch)):\n",
    "            # Convert images to channel-last format (h, w, c) for YOLO model\n",
    "            batch[i] = np.transpose(batch[i], (1, 2, 0))\n",
    "\n",
    "        yolo_predictions = self._model(batch, verbose=False)\n",
    "        return [\n",
    "            YOLODetectionTarget(\n",
    "                p.boxes.xyxy.cpu(),  # Bounding boxes in (x_min, y_min, x_max, y_max) format\n",
    "                p.boxes.cls.cpu(),  # Class indices for the detected objects\n",
    "                p.boxes.conf.cpu(),  # Confidence scores for the detections\n",
    "            )\n",
    "            for p in yolo_predictions\n",
    "        ]\n",
    "\n",
    "\n",
    "# create the wrapped model object\n",
    "yolo_model: od.Model = MaiteYOLODetector(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef0788-137f-41c1-a9e4-80e7d08b47d1",
   "metadata": {},
   "source": [
    "## (2) Wrapping the dataset\n",
    "\n",
    "MAITE pairs images and their reference detections (aka targets, ground truth) into **datasets**. Typical ML workflows have many images per dataset; when these do not all fit in memory simultaneously, a *dataloader* object is used which can page images and annotations in from disk. For this notebook, however, each invocation of `evaluate` will use the same single-image dataset (our reference image with its baseline detections.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbb5d0-b899-4355-9922-9d52fb866b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our single image, its baseline detections, and a metadata dictionary\n",
    "# switch image to channel first\n",
    "single_image_dataset: od.Dataset = JATICObjectDetectionDataset(imgs, dets, metadata, dataset_id=\"visdrone_ex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963f7fa-052c-476d-944e-5bbdae3181b5",
   "metadata": {},
   "source": [
    "## (3) Wrapping the perturbations as augmentations\n",
    "\n",
    "The `evaluate` function will perturb the image from the dataset using instances of the class defined below, one instance per perturbation value. Note that the object doesn't perform any augmentations until called by the `evaluate` workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrtk.interop.maite.interop.object_detection.augmentation import JATICDetectionAugmentation\n",
    "\n",
    "bp = BrightnessPerturber(factor=1.0)\n",
    "identity_augmentation = JATICDetectionAugmentation(bp, augment_id=\"identity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81020be7-76ba-43e3-b231-7d2bd7c30387",
   "metadata": {},
   "source": [
    "## (4) Wrapping the metrics\n",
    "\n",
    "We'll compare the detections in each perturbed image to the unperturbed detections using the Mean Average Precision (mAP) metric from the `torchmetrics` package. The following cell creates a mAP metrics object, wraps it in a MAITE [MAITE Object Detection Metric](https://jatic.pages.jatic.net/cdao/maite/generated/maite.protocols.object_detection.Metric.html) protocol-compatible class, and then creates an instance of this class, which will be called by `evaluate`.\n",
    "\n",
    "This code is copied directly from the [MAITE object detection tutorial](https://jatic.pages.jatic.net/cdao/maite/tutorials/torchvision_object_detection.html#metrics) (with the exception of setting `class_metrics=True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33407c5-f1d7-4c7a-9a31-2d500f97edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "##\n",
    "## Create an instance of the MAP metric object\n",
    "##\n",
    "\n",
    "tm_metric = MeanAveragePrecision(\n",
    "    box_format=\"xyxy\",\n",
    "    iou_type=\"bbox\",\n",
    "    iou_thresholds=[0.5],\n",
    "    rec_thresholds=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    max_detection_thresholds=[1, 10, 100],\n",
    "    class_metrics=True,\n",
    "    extended_summary=False,\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "##\n",
    "## This wrapper associates the MAP metric object with methods called by the evaluate\n",
    "## workflow to accumulate detection data and compute the metrics.\n",
    "##\n",
    "\n",
    "\n",
    "class WrappedTorchmetricsMetric:\n",
    "    \"\"\"\n",
    "    A wrapper class for a Torchmetrics metric designed to simplify its usage for object detection tasks.\n",
    "\n",
    "    This class facilitates the conversion of object detection targets and predictions into the format\n",
    "    expected by Torchmetrics metrics, allowing for easier integration with existing pipelines.\n",
    "\n",
    "    Attributes:\n",
    "        _tm_metric (Callable): The Torchmetrics metric to be wrapped, which takes lists of dictionaries\n",
    "            containing torch.Tensor objects representing predictions and targets.\n",
    "\n",
    "    Methods:\n",
    "        to_tensor_dict(target):\n",
    "            Converts an `ObjectDetectionTarget` into a dictionary format compatible with the Torchmetrics\n",
    "            metric's `update` method.\n",
    "\n",
    "        update(preds, targets):\n",
    "            Updates the wrapped Torchmetrics metric with batches of predictions and targets in their native format.\n",
    "\n",
    "        compute():\n",
    "            Computes the final metric values using the wrapped Torchmetrics metric.\n",
    "\n",
    "        reset():\n",
    "            Resets the state of the wrapped Torchmetrics metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tm_metric: Callable[[list[dict[str, torch.Tensor]], list[dict[str, torch.Tensor]]], dict[str, Any]],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the WrappedTorchmetricsMetric with the given Torchmetrics metric.\n",
    "\n",
    "        Args:\n",
    "            tm_metric (Callable): A Torchmetrics metric instance that expects predictions and targets as lists of\n",
    "                dictionaries containing torch.Tensor objects.\n",
    "        \"\"\"\n",
    "        self._tm_metric = tm_metric\n",
    "\n",
    "    @staticmethod\n",
    "    def to_tensor_dict(target: od.ObjectDetectionTarget) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Converts an ObjectDetectionTarget into a dictionary format compatible with the Torchmetrics metric's\n",
    "        `update` method.\n",
    "\n",
    "        Args:\n",
    "            target (od.ObjectDetectionTarget): An object detection target instance containing boxes, labels, and scores.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, torch.Tensor]: A dictionary with keys `boxes`, `scores`, and `labels`, each mapping to a tensor.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"boxes\": torch.as_tensor(target.boxes),\n",
    "            \"scores\": torch.as_tensor(target.scores),\n",
    "            \"labels\": torch.as_tensor(target.labels).type(torch.int64),\n",
    "        }\n",
    "\n",
    "    def update(self, preds: od.TargetBatchType, targets: od.TargetBatchType) -> None:\n",
    "        \"\"\"\n",
    "        Updates the wrapped Torchmetrics metric with the given predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            preds (od.TargetBatchType): A batch of predictions in the format expected by the Torchmetrics metric.\n",
    "            targets (od.TargetBatchType): A batch of targets in the format expected by the Torchmetrics metric.\n",
    "        \"\"\"\n",
    "        preds_tm = [self.to_tensor_dict(pred) for pred in preds]\n",
    "        targets_tm = [self.to_tensor_dict(tgt) for tgt in targets]\n",
    "        self._tm_metric.update(preds_tm, targets_tm)\n",
    "\n",
    "    def compute(self) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Computes and returns the final metric values using the wrapped Torchmetrics metric.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, Any]: A dictionary containing the computed metric values.\n",
    "        \"\"\"\n",
    "        return self._tm_metric.compute()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the state of the wrapped Torchmetrics metric, clearing any accumulated data.\"\"\"\n",
    "        self._tm_metric.reset()\n",
    "\n",
    "\n",
    "##\n",
    "## This is our instance variable that can compute the MAP metrics.\n",
    "##\n",
    "\n",
    "mAP_metric: od.Metric = WrappedTorchmetricsMetric(tm_metric)  # noqa: N816"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102dfde-cb9b-46a7-8e4a-d12ccd948864",
   "metadata": {},
   "source": [
    "# Running the evaluation\n",
    "\n",
    "We now have all the wrappings required to evaluate our range of perturbations:\n",
    "- The `yolo_model` object, wrapping the YOLO model\n",
    "- The `single_image_dataset` object, providing our source image and its baseline detections\n",
    "- The `augmentation` object, which when instantiated, applies a single perturbation value to its input\n",
    "- The `mAP_metrics` object, defining the metrics to compute at each perturbation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7fef1-a3cc-4e63-8a2b-6133274e4b1c",
   "metadata": {},
   "source": [
    "## Evaluation sanity check: ground truth against itself\n",
    "\n",
    "Here we quickly check the evaluation workflow by creating an *identity augmentation* (with a brightness perturbation factor of 1.0, leaving the image unchanged) and scoring it.  The detections should also be unchanged from the baseline and thus give an mAP of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af9f36-c69b-400f-82ec-f1fd7e9d98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maite.workflows import evaluate\n",
    "\n",
    "# call the model for each image in the dataset (in this case, just the source image),\n",
    "# scoring the resulting detections against those from the dataset\n",
    "sanity_check_results, _, _ = evaluate(\n",
    "    model=yolo_model,\n",
    "    dataset=single_image_dataset,\n",
    "    augmentation=identity_augmentation,\n",
    "    metric=mAP_metric,\n",
    ")\n",
    "\n",
    "print(\"Sanity check: overall mAP (should be 1.0):\", sanity_check_results[\"map\"].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a72bb-dbf1-431e-892e-132771cd7b92",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Now we'll prepare the augmentation instances for the evaluation. In the cell below, you can set three parameters for sweeping the set of perturbation values:\n",
    "- **sweep_low**: the minimum perturbation value (must be >= 0)\n",
    "- **sweep_high**: the maximum perturbation value\n",
    "- **sweep_count**: how many perturbations to generation\n",
    "\n",
    "You can also optionally select perturbations to visualize:\n",
    "- **visualization_indices**: a list of perturbation indices *p*, 0 <= *p* < sweep_count. These instances will be rendered along with their corresponding detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5e334-d8eb-415c-b263-4c4bd3618b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "SWEEP_LOW = 0.2\n",
    "SWEEP_HIGH = 2.0\n",
    "SWEEP_COUNT = 30\n",
    "VISUALIZATION_INDICES = [3, 9, 21]\n",
    "\n",
    "##\n",
    "## end user-settable parameters\n",
    "##\n",
    "\n",
    "perturbation_values = np.linspace(SWEEP_LOW, SWEEP_HIGH, SWEEP_COUNT, endpoint=True)\n",
    "augmentations = [\n",
    "    JATICDetectionAugmentation(BrightnessPerturber(p), augment_id=idx) for idx, p in enumerate(perturbation_values)\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(augmentations)} perturbation augmentations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d0d4d-eabd-4f54-a487-7cfbc99916c9",
   "metadata": {},
   "source": [
    "## Calling evaluate on the augmented data\n",
    "\n",
    "We loop over all the augmentations, calling `evaluate` on each one and building up a list of resulting metrics for analysis.\n",
    "\n",
    "Any augmentation indices specified above will be rendered in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4951dfc-c2cc-434a-8fd2-1786ba3acf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_metrics = list()\n",
    "for idx, a in enumerate(augmentations):\n",
    "    # reset the metric object for each dataset\n",
    "    mAP_metric.reset()\n",
    "    result, _, _ = evaluate(model=yolo_model, dataset=single_image_dataset, augmentation=a, metric=mAP_metric)\n",
    "    perturbed_metrics.append(result)\n",
    "\n",
    "    if idx in VISUALIZATION_INDICES:\n",
    "        # quickest way is to re-evaluate\n",
    "        print(f\"Perturbation #{idx}: brightness perturbation value {a.augment.factor:0.5}\")\n",
    "        datum = single_image_dataset[0]\n",
    "        batch = ([datum[0]], [datum[1]], [datum[2]])\n",
    "        # Extract the image from the augmentation and switch it to channel last\n",
    "        aug = np.transpose(a(batch)[0][0], (1, 2, 0))\n",
    "        _ = model(aug)[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0303b6a-f587-43c8-ae9f-0057a7884241",
   "metadata": {},
   "source": [
    "# Evaluation analysis\n",
    "\n",
    "Now we can plot how the metrics (for example, mAP @ IoU=50) vary with perturbation level, keeping in mind this is a **relative** mAP against the detections in the unperturbed image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6804b-c98c-46fb-af98-ab5598892ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "map50_list = [m[\"map_50\"].item() for m in perturbed_metrics]\n",
    "plt.title(\"relative mAP@50\")\n",
    "plt.xlabel(\"Brightness perturbation factor\")\n",
    "plt.ylabel(\"relative mAP @ 50% IoU\")\n",
    "_ = plt.plot(perturbation_values, map50_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8720fd-32dc-493d-ab5c-81c313615504",
   "metadata": {},
   "source": [
    "## Evaluation interpretation\n",
    "\n",
    "Note that as plotted, the minimum y-axis value is 0.4. The metric shown, mAP@50, is the average precision of detections across all classes when the bounding box IoU is at least 0.5 (for more details, [see here](https://lightning.ai/docs/torchmetrics/stable/detection/mean_average_precision.html).) In general, we observe a perturbation value range between around 0.6 and 1.6 where the score is about 0.95 or higher, and sharp falloffs when the perturbation factor is below roughly 0.4 or above 1.75.  (Note the relative mAP is guaranteed to be 1.0 when the perturbation is 1.0, i.e. when the image is unchanged, the two detection sets are identical.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0f9b97-ce54-4d21-be91-160f4a5d3054",
   "metadata": {},
   "source": [
    "## Additional plots\n",
    "\n",
    "For further insight, we can plot the mAP per class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d28ff5-838e-467a-a703-81432a408b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Each instance of the metrics object has, potentially, a different set of observed classes.\n",
    "# Loop through them to accumulate a unified set of classes to ensure consistent plotting across\n",
    "# all thresholds.\n",
    "#\n",
    "\n",
    "unified_classes = set()\n",
    "for m in perturbed_metrics:\n",
    "    for class_idx in m[\"classes\"].tolist():\n",
    "        unified_classes.add(class_idx)\n",
    "\n",
    "#\n",
    "# dictionary of class_idx -> list of per-class mAP, or 0 if not present at that threshold\n",
    "#\n",
    "\n",
    "class_mAP = {class_idx: list() for class_idx in unified_classes}  # noqa: N816\n",
    "\n",
    "#\n",
    "# populate the lists across the perturbation values\n",
    "#\n",
    "\n",
    "for m in perturbed_metrics:\n",
    "    this_perturbation_classes = m[\"classes\"].tolist()\n",
    "    for class_idx in unified_classes:\n",
    "        if class_idx in this_perturbation_classes:\n",
    "            # the index of the class in this individual metric instance\n",
    "            this_class_idx = this_perturbation_classes.index(class_idx)\n",
    "            class_mAP[class_idx].append(m[\"map_per_class\"][this_class_idx].item())\n",
    "        else:\n",
    "            class_mAP[class_idx].append(0)\n",
    "\n",
    "#\n",
    "# plot\n",
    "#\n",
    "\n",
    "plt.title(\"Relative mAP per class\")\n",
    "plt.xlabel(\"Brightness perturbation factor\")\n",
    "plt.ylabel(\"relative mAP @ 50% IoU\")\n",
    "for class_idx, class_mAP_list in class_mAP.items():  # noqa: N816\n",
    "    plt.plot(perturbation_values, class_mAP_list, label=baseline[0].names[class_idx])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef482b-1a2b-4b2e-b522-7032776320d2",
   "metadata": {},
   "source": [
    "This plot shows several interesting results:\n",
    "\n",
    "- The only \"false alarm\" **class** (with a mAP value of -1) is the appearance of one or more bicycles around 1.24. There are no bicycles in the \"ground truth\" unperturbed detections (but again, since this a relative mAP, this does not necessarily mean there are no true bicycles in the image.)\n",
    "\n",
    "- The truck, person, and motorcycle classes are much more robust to brightness perturbations than the car class. Examination of the unperturbed and a sample low-illumination image suggest a few possibilities:\n",
    "\n",
    "    - There are many more car instances (15) than non-car (5 persons, 1 motorcycle, 2 trucks) in the unperturbed image.\n",
    "    - The non-car classes detected in the unperturbed image are all in the foreground, and are thus larger. People, in particular, are distributed throughout the image, but only detected when relatively close and large.\n",
    "\n",
    "Any conclusions about classification accuracy should be considered in light of these caveats. In particular, the foreground positioning of the detected non-car objects suggests that instead of looking at per-class results, we drill down by bounding box area. Fortunately, the metrics class supports this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec21e7-e682-4d08-ad1d-6c42d850402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"relative mAP values per area\")\n",
    "plt.xlabel(\"Brightness perturbation factor\")\n",
    "plt.ylabel(\"relative mAP\")\n",
    "for k in (\"map\", \"map_small\", \"map_medium\"):\n",
    "    plt.plot(perturbation_values, [m[k].item() for m in perturbed_metrics], label=k)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e276bd-2eaf-4a4c-a3cd-9bb8c98fa30c",
   "metadata": {},
   "source": [
    "The `map` line covers all sizes; `map_small` and `map_medium` are the mean average precision for objects (smaller than 32^2 pixels, between 32^2 and 96^2 pixels) in area, respectively. (There are no detections in the `map_large` category.) (Here, the mAP value is averaged over a **range** of IoU thresholds, between 0.5 and 0.95.) We see that medium objects, regardless of class, are generally much more robust to brighter illumination perturbations than small ones, which makes sense when observing how objects tend to get \"washed out\" in the perturbation examples shown in the [Examples and Guidance](#NRTK-Brightness-perturbation:-examples-and-guidance) section above. For dimming perturbations, the situation is reversed (although not as dramatically): small objects tend to be more robust than medium, but both fall off dramatically around 0.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b9f2a-7217-4427-9c4f-f84656a4dcf2",
   "metadata": {},
   "source": [
    "# End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
