{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metric Example Workflow\n",
    "\n",
    "In this notebook we showcase how a fictional T&E engineer, Jack, could utilize NRTK and the ImageMetric interface in a workflow.\n",
    "\n",
    "Jack has identified that the quality of images his model will be receiving is likely to degrade due to a variety of natural pertubations beyond what was initially expected. He wants to use NRTK to explore the direct relationship between image quality and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # noqa: F401\n",
    "\n",
    "!{sys.executable} -m pip install -qU pip\n",
    "print(\"Installing torch and torchvision...\")\n",
    "!{sys.executable} -m pip install -q \"torch!=2.0.1\" \"torchvision==0.17\"\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"jpeg\"  # Use JPEG format for inline visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import urllib\n",
    "from collections.abc import Callable, Iterable, Iterator, Sequence\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import maite.protocols.object_detection as od\n",
    "import numpy as np\n",
    "import torch\n",
    "from maite.protocols import ArrayLike\n",
    "from maite.workflows import evaluate\n",
    "from matplotlib import pyplot as plt  # type: ignore\n",
    "from models.datasets import VisDroneDataset\n",
    "from numpy.typing import NDArray\n",
    "from smqtk_detection.impls.detect_image_objects.centernet import CenterNetVisdrone\n",
    "from smqtk_detection.interfaces.detect_image_objects import DetectImageObjects\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "from nrtk.impls.image_metric.snr_image_metric import SNRImageMetric\n",
    "from nrtk.impls.perturb_image.generic.cv2.blur import AverageBlurPerturber\n",
    "from nrtk.impls.perturb_image.generic.PIL.enhance import BrightnessPerturber\n",
    "from nrtk.impls.perturb_image.generic.skimage.random_noise import PepperNoisePerturber\n",
    "from nrtk.interop.maite.interop.object_detection.augmentation import JATICDetectionAugmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jack will be using the maite evaluate functionality to perform his model evaluation. In order to use that, he needs to set up a number of maite compliant objects (Metric, Dataset, and Model). He starts by loading in a portion of the Visdrone Dataset into his maite compiant Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ObjectDetectionData:\n",
    "    \"\"\"Dataclass for object detection.\"\"\"\n",
    "\n",
    "    boxes: ArrayLike\n",
    "    labels: ArrayLike\n",
    "    scores: ArrayLike\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[ArrayLike, ArrayLike, ArrayLike]]:\n",
    "        \"\"\"Return iterator of ObjectDetectionData.\"\"\"\n",
    "        self.n = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"Return next ObjectDetectionData in iterator.\"\"\"\n",
    "        if self.n < len(self.boxes):\n",
    "            self.n += 1\n",
    "            return self.boxes[self.n - 1].astype(np.float32), self.labels[self.n - 1], self.scores[self.n - 1]\n",
    "        raise StopIteration\n",
    "\n",
    "\n",
    "DEMO_ROOT = Path.cwd().parent\n",
    "basic_dataset: od.Dataset = VisDroneDataset(DEMO_ROOT / \"examples/data\" / \"VisDrone2019-DET-test-dev-TINY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Jack defines a metric which he wants to use for a quantitative evaluation of his model. He chooses to use the Mean Average Precision metric implemented in `torchmetrics`, and he wraps that in a class that complies with the maite Metric protocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_metric = MeanAveragePrecision(\n",
    "    box_format=\"xyxy\",\n",
    "    iou_type=\"bbox\",\n",
    "    iou_thresholds=[0.5],\n",
    "    rec_thresholds=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    max_detection_thresholds=[1, 10, 100],\n",
    "    class_metrics=False,\n",
    "    extended_summary=False,\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "\n",
    "class WrappedTorchmetricsMetric:\n",
    "    \"\"\"MAITE wrapper for torch metric.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tm_metric: Callable[[list[dict[str, torch.Tensor]], list[dict[str, torch.Tensor]]], dict[str, Any]],\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize wrapped torch metric.\"\"\"\n",
    "        self._tm_metric = tm_metric\n",
    "\n",
    "    # Create utility function to convert ObjectDetectionTarget_impl type to what\n",
    "    # the type expected by torchmetrics IntersectionOverUnion metric\n",
    "    @staticmethod\n",
    "    def to_tensor_dict(target: od.ObjectDetectionTarget) -> dict[str, torch.Tensor]:\n",
    "        \"\"\"Convert an ObjectDetectionTarget_impl into an expected dictionary.\"\"\"\n",
    "        return {\n",
    "            \"boxes\": torch.as_tensor(target.boxes),\n",
    "            \"scores\": torch.as_tensor(target.scores),\n",
    "            \"labels\": torch.as_tensor(target.labels),\n",
    "        }\n",
    "\n",
    "    def update(self, preds: od.TargetBatchType, targets: od.TargetBatchType) -> None:\n",
    "        \"\"\"Update predictions and targets for metric.\"\"\"\n",
    "        # Convert to natively-typed from of preds/targets\n",
    "        preds_tm = [self.to_tensor_dict(pred) for pred in preds]\n",
    "        targets_tm = [self.to_tensor_dict(tgt) for tgt in targets]\n",
    "        self._tm_metric.update(preds_tm, targets_tm)\n",
    "\n",
    "    def compute(self) -> dict[str, Any]:\n",
    "        \"\"\"Compute metric.\"\"\"\n",
    "        return self._tm_metric.compute()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset metric.\"\"\"\n",
    "        self._tm_metric.reset()\n",
    "\n",
    "\n",
    "map_metric: od.Metric = WrappedTorchmetricsMetric(tm_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly he loads the centernet model he wants to evaluate and creates a wrapper class for the model that complies with the maite Model protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SMQTKModelMetadata:\n",
    "    \"\"\"This is the implementation of ModelMetadata protocol.\"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    provider: str\n",
    "    task: str\n",
    "    model_info: dict  # model_info is the user defined metadata\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SMQTKObjectDetectionOutput:\n",
    "    \"\"\"Dataclass for SMQTK object detection.\"\"\"\n",
    "\n",
    "    boxes: Sequence[NDArray]\n",
    "    labels: Sequence[NDArray]\n",
    "    scores: Sequence[NDArray]\n",
    "\n",
    "\n",
    "def _get_top_label_score(label2score: dict) -> tuple[Any, float]:\n",
    "    k = list(label2score.keys())\n",
    "    v = list(label2score.values())\n",
    "    max_v = max(v)\n",
    "    return k[v.index(max_v)], max_v\n",
    "\n",
    "\n",
    "class SMQTKObjectDetector:\n",
    "    \"\"\"Wraps SMQTK `DetectImageObjects` as MAITE `ObjectDetector`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smqtk_detector : DetectImageObjects\n",
    "        The SMQTK bject detector to wrap.\n",
    "    labels : Sequence[str]\n",
    "        Labels for classes that object detector can detect.\n",
    "    model_name: str\n",
    "        The name of the model, used for model's metadata\n",
    "    map_output_labels : bool\n",
    "        Whether wrapper needs to map string outputs from SMQTK detector to integers.\n",
    "    metadata: SMQTKModelMetadata\n",
    "        Model metadata\n",
    "    \"\"\"\n",
    "\n",
    "    metadata: SMQTKModelMetadata\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        smqtk_detector: DetectImageObjects,\n",
    "        labels: Sequence[str],\n",
    "        map_output_labels: bool,\n",
    "        metadata: SMQTKModelMetadata,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize SMQTKObjectDetector.\"\"\"\n",
    "        self.smqtk_detector = smqtk_detector\n",
    "        self.labels = labels\n",
    "        self.label2int: dict[str, int] = {label: idx for idx, label in enumerate(labels)}\n",
    "        self.map_output_labels = map_output_labels\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def get_labels(self) -> Sequence[str]:\n",
    "        \"\"\"Get labels for model.\"\"\"\n",
    "        return self.labels\n",
    "\n",
    "    def _format_imgs(self, data_seq: Sequence[ArrayLike]) -> Iterable[NDArray]:\n",
    "        \"\"\"Reformat imgs.\"\"\"\n",
    "        arr_iter: Iterable[NDArray] = []\n",
    "        for img in data_seq:\n",
    "            if isinstance(img, torch.Tensor) and len(img.shape) == 3:\n",
    "                arr_iter.append(img.detach().cpu().numpy().transpose([1, 2, 0]))\n",
    "            elif isinstance(img, np.ndarray) and len(img.shape) == 3:\n",
    "                if img.shape[0] != 3:\n",
    "                    img = img.transpose([2, 0, 1])\n",
    "                arr_iter.append(img.transpose([1, 2, 0]))\n",
    "            else:\n",
    "                raise Exception(f\"Unable to handle sequence item of type: {type(img)}\")\n",
    "\n",
    "        return arr_iter\n",
    "\n",
    "    def __call__(self, data: Sequence[ArrayLike]) -> Sequence[ObjectDetectionData]:\n",
    "        \"\"\"SMQTK DetectImageObjects.detect_objects.\n",
    "\n",
    "        - input: Iterable[ndarray]\n",
    "        - output: Iterable[Iterable[Tuple[AxisAlignedBoundingBox, Dict[Hashable, float]]]]\n",
    "        \"\"\"\n",
    "        # reformat data as sequence first if necessary\n",
    "        # tensor bridging\n",
    "        if isinstance(data, np.ndarray):\n",
    "            data_seq = list(data) if len(data.shape) == 4 else [data]\n",
    "        elif isinstance(data, Sequence):\n",
    "            # already a sequence\n",
    "            data_seq = data  # convert to Iterable[ndarray]\n",
    "\n",
    "        arr_iter = self._format_imgs(data_seq)\n",
    "        smqtk_output = self.smqtk_detector.detect_objects(arr_iter)\n",
    "        # assume num detections for image i is nd_i\n",
    "        # sequence of shape-(nd_i, 4) bounding box arrays\n",
    "\n",
    "        # sequence of shape-(nd_i,) arrays of predicted class associated with each bounding box\n",
    "\n",
    "        # sequence of shape-(nd_i,) arrays of score for predicted class associated with each bounding box\n",
    "\n",
    "        # bounding boxes, top label for each box, top score for each box for *single* image\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "\n",
    "        for bbox, label2score in smqtk_output[0]:\n",
    "            flatten_box = np.hstack([bbox.min_vertex, bbox.max_vertex])\n",
    "            boxes.append(flatten_box)\n",
    "\n",
    "            top_label, top_score = _get_top_label_score(label2score)\n",
    "            if self.map_output_labels:\n",
    "                top_label = self.label2int[top_label]\n",
    "\n",
    "            labels.append(top_label)\n",
    "            scores.append(top_score)\n",
    "\n",
    "        return [SMQTKObjectDetectionOutput(boxes=boxes, labels=labels, scores=scores)]\n",
    "\n",
    "\n",
    "# download weights if necessary\n",
    "model_dir = DEMO_ROOT / \"examples\" / \"models\"\n",
    "\n",
    "model_file = Path(model_dir) / \"centernet-resnet50.pth\"\n",
    "provider = \"Kitware: github.com/SMQTK-Detection\"\n",
    "\n",
    "if not model_file.is_file():\n",
    "    print(f\"Downloading CenterNet model checkpoint to: {model_file}\")\n",
    "    urllib.request.urlretrieve(  # type: ignore\n",
    "        \"https://data.kitware.com/api/v1/item/623259f64acac99f426f21db/download\",\n",
    "        model_file,\n",
    "    )\n",
    "\n",
    "centernet_detector = CenterNetVisdrone(\n",
    "    arch=\"resnet50\",\n",
    "    model_file=str(model_file),\n",
    "    max_dets=500,\n",
    "    use_cuda=False,\n",
    "    batch_size=1,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "metadata = SMQTKModelMetadata(\n",
    "    provider=provider,\n",
    "    model_name=\"tph-yolov5\",\n",
    "    task=\"object-detection\",\n",
    "    model_info={\n",
    "        \"model_file\": str(model_file.relative_to(model_file.parents[3])),\n",
    "        \"arch\": \"resnet50\",\n",
    "        \"max_dets\": 500,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "jatic_detector = SMQTKObjectDetector(\n",
    "    centernet_detector,\n",
    "    VisDroneDataset.classes,\n",
    "    map_output_labels=True,\n",
    "    metadata=metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the appropriate set of maite compliant object, Jack is ready to perform the maite evaluation across a number of generic perturbations. However in addition to the raw model performance for a given set of pertubations, Jack wants a quantifiable measure of how much each pertubation changes the image in order to generalize accross different pertubations. In order to quantify how much a given pertubation modifies the image, he decides to use the Signal to Noise Ratio implementation of the ImageMetric class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final set up for his experiments, Jack sets the random seeds for consistency and defines the image quality metric he wants to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_quality_metric = SNRImageMetric()\n",
    "\n",
    "random.seed(42)\n",
    "np_random = np.random.default_rng(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first experiment Jack wants to run is using the PepperNoisePerturber that adds black noise into the image. He evaluates the model across an incresing degree of noise in the image keeping track of the model performance metric and the image quality metric for each pertubation parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pepper_noise_img_quality_metrics = []\n",
    "pepper_noise_performance = []\n",
    "basic_dataset.set_reshape(False)\n",
    "\n",
    "for amount in [i * 0.05 for i in range(15)]:\n",
    "    perturber = PepperNoisePerturber(amount=amount)\n",
    "    augmentation = JATICDetectionAugmentation(perturber)\n",
    "    results, _, _ = evaluate(model=jatic_detector, metric=map_metric, dataset=basic_dataset, augmentation=augmentation)\n",
    "    pepper_noise_performance.append(results[\"map\"])\n",
    "    avg_quality = 0\n",
    "    for batch in basic_dataset:\n",
    "        aug_img, _, _ = augmentation([[component] for component in batch])\n",
    "        aug_img = np.transpose(aug_img[0], (1, 2, 0))\n",
    "        avg_quality += img_quality_metric.compute(aug_img)\n",
    "    pepper_noise_img_quality_metrics.append(avg_quality / len(basic_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After collecting all of the metrics, Jack first wants to confirm that there is a relationship between the image metric, and the amount of noise being applied to the image. He is able to see that as the noise in the image increases, the signal to noise metric decreases. With this knowledge he then looks at the relationship between the image metric and the model performance. Now he finds that the model performance increases with the signal to noise image metric. If his baseline performance for this model is 0.15, then he knows that he needs a signal to noise ratio of at least 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "ax1.plot([i * 0.05 for i in range(15)], pepper_noise_img_quality_metrics)\n",
    "ax1.set_title(\"Pepper Noise parameter vs Image Quality metric\")\n",
    "ax1.set_xlabel(\"Pepper Noise Parameter\")\n",
    "ax1.set_ylabel(\"Signal to Noise Ratio\")\n",
    "\n",
    "ax2.plot(pepper_noise_img_quality_metrics, pepper_noise_performance)\n",
    "ax2.axhline(y=0.15, color=\"red\", linestyle=\"--\")\n",
    "ax2.set_title(\"Performance vs Image Quality metric as Pepper Noise is applied\")\n",
    "ax2.set_xlabel(\"Signal to Noise Ratio\")\n",
    "ax2.set_ylabel(\"mean ClassAgnosticPixelwiseIoU\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)  # Increase wspace to add more horizontal space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Jack wants to perform the same experiment but adding in a brightness parameterinstead of pepper noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightness_img_quality_metrics = []\n",
    "brightness_performance = []\n",
    "basic_dataset.set_reshape(True)\n",
    "for factor in [i * 0.1 for i in range(1, 16)]:\n",
    "    perturber = BrightnessPerturber(factor=factor)\n",
    "    augmentation = JATICDetectionAugmentation(perturber)\n",
    "    results, _, _ = evaluate(model=jatic_detector, metric=map_metric, dataset=basic_dataset, augmentation=augmentation)\n",
    "    brightness_performance.append(results[\"map\"])\n",
    "    avg_quality = 0\n",
    "    for batch in basic_dataset:\n",
    "        aug_img, _, _ = augmentation([[component] for component in batch])\n",
    "        aug_img = np.array(aug_img[0])\n",
    "        avg_quality += img_quality_metric.compute(aug_img)\n",
    "    brightness_img_quality_metrics.append(avg_quality / len(basic_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After anaylysing this set of results, Jack sees that the brightness parameter has mush less of an impact on the Signal to Noise Ratio. The Image metric isn't monotonically increasing like the previous experiment however, so Jack notes the behaviour when the imagemetric is between 2.40 and 2.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "ax1.plot([i * 0.1 for i in range(1, 16)], brightness_img_quality_metrics)\n",
    "ax1.set_title(\"Brightness parameter vs Image Quality metric\")\n",
    "ax1.set_xlabel(\"Brightness Parameter\")\n",
    "ax1.set_ylabel(\"Signal to Noise Ratio\")\n",
    "\n",
    "ax2.plot(brightness_img_quality_metrics, brightness_performance)\n",
    "ax2.set_title(\"Performance vs Image Quality metric as Average Blur is applied\")\n",
    "ax2.set_xlabel(\"Signal to Noise Ratio\")\n",
    "ax2.set_ylabel(\"Mean Average Precision\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)  # Increase wspace to add more horizontal space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally Jack wants to perform the experiment with a blur pertubation\\."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_img_quality_metrics = []\n",
    "blur_performance = []\n",
    "basic_dataset.set_reshape(True)\n",
    "for ksize in range(151, 0, -10):\n",
    "    perturber = AverageBlurPerturber(ksize=ksize)\n",
    "    augmentation = JATICDetectionAugmentation(perturber)\n",
    "    results, _, _ = evaluate(model=jatic_detector, metric=map_metric, dataset=basic_dataset, augmentation=augmentation)\n",
    "    blur_performance.append(results[\"map\"])\n",
    "    avg_quality = 0\n",
    "    for batch in basic_dataset:\n",
    "        aug_img, _, _ = augmentation([[component] for component in batch])\n",
    "        aug_img = np.array(aug_img[0])\n",
    "        avg_quality += img_quality_metric.compute(aug_img)\n",
    "    blur_img_quality_metrics.append(avg_quality / len(basic_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "ax1.plot(list(range(151, 0, -10)), blur_img_quality_metrics)\n",
    "ax1.set_title(\"Average Blur parameter vs Image Quality metric\")\n",
    "ax1.set_xlabel(\"Average Blur Parameter\")\n",
    "ax1.set_ylabel(\"Signal to Noise Ratio\")\n",
    "\n",
    "ax2.plot(blur_img_quality_metrics, blur_performance)\n",
    "ax2.axhline(y=0.15, color=\"red\", linestyle=\"--\")\n",
    "ax2.set_title(\"Performance vs Image Quality metric as Average Blur is applied\")\n",
    "ax2.set_xlabel(\"Signal to Noise Ratio\")\n",
    "ax2.set_ylabel(\"Mean Average Precision\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5)  # Increase wspace to add more horizontal space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that Jack has performed his experiments on three different pertubations, he wants to compare the results in order to find out what range of signal to noise ratio equates to acceptable performance of his model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pepper_noise_img_quality_metrics, pepper_noise_performance, label=\"pepper noise\")\n",
    "plt.plot(blur_img_quality_metrics, blur_performance, label=\"blur\")\n",
    "plt.plot(brightness_img_quality_metrics, brightness_performance, label=\"brightness\")\n",
    "plt.axhline(y=0.15, color=\"red\", linestyle=\"--\")\n",
    "plt.legend()\n",
    "plt.title(\"Perturbation Image Metric Comparison\")\n",
    "plt.xlabel(\"Signal to Noise Ratio\")\n",
    "plt.ylabel(\"Mean Average Precision\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the performance vs image metric graphs, Jack concludes that when the image metric falls below 2.0 or above 2.7, the model performance drops past the acceptable level. He can then advise specific monitoring of the image quality metric that is collected to identify when the model is likely to suffer in quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
